---
title: "Outlier detection from raw text"
author: "Gabriel Pi≈°vejc"
format: pdf
---

```{r Prepare the env}
#| include: false

library(tidyverse)
library(tidymodels)
library(tm)
library(dbscan)
library(isotree)

rm(list = ls())

```

```{r Load data}
#| include: false
data <- read.csv("data/2/data_rt.csv")

# And shuffle it, since the first half of the dataset is just negative reviews, and the second half is just positive. 
data <- data[sample(nrow(data)), ]

```

# Bag of words preprocessing

In this experiment, we will be playing with binary sentiment analysis on movie reviews. The data we have has 2 columns, in the first one, we have the reviews, in the second, either a 1 or a 0 to show whether they are positive or negative. Given the size of the dataset, (~10k reviews), the hardware I have with me (CPU) and the scope of this homework, I will only be working with Bag of words preprocessing. 

For that, we can run the following code, where we first put all of the words to lower case, we remove punctuation and strip white-space irregularities. We then generate a document-term matrix and attach the labels back to it as a factor.

```{r Bag of words preprocessing}
#| echo: false
corpus <- Corpus(VectorSource(data$reviews))

corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)

BoW_data <- DocumentTermMatrix(corpus)
BoW_data <- BoW_data %>% as.matrix() %>% as_tibble()

BoW_data <- cbind(BoW_data, data$labels)

colnames(BoW_data)[colnames(BoW_data) == "data$labels"] <- "labels_target"

BoW_data$labels_target <- 
  BoW_data$labels_target %>%
  as.factor()

```

## Baseline model quality evaluation

For each outlier handling method, we will train an XGBoost model with 500 trees of 6 deep on 80% of the available data and test it on the remaining 20%. Then, we will observe the AUC-score of the model and see whether removing data was actually any helpful and if so, then by how much. Each time we do this, we will take advantage of the following function. 

```{r Define the diagnostic function}

test_xgb <- 
  function(input){
    
    data <- input 
      
    set.seed(1989)
    
    split <- initial_split(data,
                           prop = 0.8,
                           strata = labels_target)
    
    train <- training(split)
    test  <- testing(split)
    
    rec <- recipe(labels_target ~ ., data = train)
    
    xgb_model <- boost_tree(
      trees = 500,
      tree_depth = 6, 
      learn_rate = 0.1,
      loss_reduction = 0,
      sample_size = 1) %>%
      set_mode("classification") %>%
      set_engine("xgboost")
    
    wf <- 
      workflow() %>%
      add_recipe(rec) %>%
      add_model(xgb_model)
    
    fit <- fit(wf, data = train)
    
    
    pred <- predict(fit, test, type = "prob")
    
    results <- test %>%
      select(labels_target) %>%
      bind_cols(pred)
    
    
    roc_res <- roc_curve(results, truth = labels_target, .pred_0)
    
    auc <- roc_auc(results, truth = labels_target, .pred_0)[".estimate"] %>% pull()
    
    
    ggplot(roc_res,
           aes(x = 1 - specificity, y = sensitivity)) +
           geom_ribbon(aes(ymin = 0, ymax = sensitivity), fill = "cyan3", alpha = 0.3) +
           geom_line(color = "orange2") +
           geom_abline(lty = 2, color = "gray") +
           labs(title = "ROC Curve",
                 x = "False Positive Rate",
                 y = "True Positive Rate") +
           geom_label(aes(x = 0.85, y = 0.1,
                           label = paste0("AUC = ", round(auc, 3))),
                       size = 4) +
           theme_bw() 
      
}

```

And since we are interested in removing outliers in order to improve the quality of our ML models, we need to see the performance before any outliers have been removed. 

```{r Get a baseline}

test_xgb(BoW_data)

```




<!-- I will not be using an autoencoder there is like 20k features and 10k observations. I only have a CPU on the laptop I brought with me for Christmas... -->

# LOF outlier detection

Next up, we can use the local outlier factor scores (LOFs) to see if any reviews are way too unique. We will first separate the 2 groups in the **training data** and then, run the LOF algorithm on each group, noting down the scores for each point and taking a look at the PDF of the resulting distribution. 

```{r lof}

negative <- 
  BoW_data %>% 
  filter(labels_target == "0")

negative$lof_index <- lof(negative %>% select(-labels_target))

negative %>% 
  select(lof_index) %>%
  pull() %>%
  density() %>%
  plot(main = "negative")

positive <- 
  BoW_data %>% 
  filter(labels_target == "1")

positive$lof_index <- lof(positive %>% select(-labels_target))

positive %>% 
  select(lof_index) %>%
  pull() %>%
  density() %>%
  plot(main = "positive")


```

We see that bath the positive and negative distributions are normal-ish, however, both show a strong positive skew. The point where we make the cut is however bound to be arbitrary, therefore, we will just decide to keep the 99.5% of the data. 

```{r remove the highest scores}

negative <- negative[negative$lof_index < quantile(negative$lof_index, 0.995),]

positive <- positive[positive$lof_index < quantile(positive$lof_index, 0.995),]

```

```{r Put everything back together}
#| include: false

negative$labels_target <- 0
positive$labels_target <- 1

lof_cleaned_data <- rbind(negative, positive)

lof_cleaned_data <- lof_cleaned_data[sample(nrow(lof_cleaned_data)), ]

lof_cleaned_data$labels_target <- lof_cleaned_data$labels_target %>%
  as.factor() 

```

Once the data is put back together, we can observe the impact on the AUC. 

```{r}

test_xgb(lof_cleaned_data)

```

# IsoForest outlier detection

In a very similar fashion, we can try to find out the most unique reviews using the isolation forest model, separating positives and negatives in the **Training data** and noting down the score

```{r Isoforest}

negative <- 
  BoW_data %>% 
  filter(labels_target == "0")

model <- isolation.forest(negative %>% select(-labels_target))

negative$iso_index <- predict(model, negative %>% select(-labels_target)) %>%
  as.numeric()

negative %>% 
  select(iso_index) %>%
  pull() %>%
  density() %>%
  plot(main = "negative")

positive <- 
  BoW_data %>% 
  filter(labels_target == "1")

model <- isolation.forest(positive %>% select(-labels_target))

positive$iso_index <- predict(model, positive %>% select(-labels_target)) %>%
  as.numeric()

positive %>% 
  select(iso_index) %>%
  pull() %>%
  density() %>%
  plot(main = "positive")


```

While the distributions resemble a Poisson distribution a bit more then the previous ones, the cut-off point remains completely arbitrary. Therefore, we will use the same quantile as before. 

```{r remove the highest scores 2}

negative <- negative[negative$iso_index < quantile(negative$iso_index, 0.995),]

positive <- positive[positive$iso_index < quantile(positive$iso_index, 0.995),]

```

```{r Put everything back together}
#| include: false

negative$labels_target <- 0
positive$labels_target <- 1

iso_cleaned_data <- rbind(negative, positive)

iso_cleaned_data <- iso_cleaned_data[sample(nrow(iso_cleaned_data)), ]

iso_cleaned_data$labels_target <- iso_cleaned_data$labels_target %>%
  as.factor() 

```

And once again, we can observe the effect on the AUC. 

```{r}

test_xgb(iso_cleaned_data)

```

# Conclusions

While by no means exhaustive, this report hopes to show the importance of outlier handling. In order to give a statistically valid proof... **Write something not ironic**

